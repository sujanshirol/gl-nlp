{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Encoder-Decoderipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sait1eIaxmGY"
      },
      "source": [
        "#inherit parent class: tf.keras.Model\n",
        "class Encoder(tf.keras.Model):\n",
        "    # input params for the class\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        \n",
        "        # calling parent initiator from it has been inheried\n",
        "        # tf.keras.Model is being iniatiated\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # initiating params\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "\n",
        "        # embeding is the 1st layer of encoder network\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # iniating GRU. We can use RNN or LSTM also\n",
        "        # return_sequences: return output of each state i.e., each h0\n",
        "        # return_state: the final context\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units, \n",
        "                                    return_sequences=True, \n",
        "                                    return_state=True, \n",
        "                                    recurrent_initializer='glorot_uniform')\n",
        "    \n",
        "    # building the above model\n",
        "    # x: input\n",
        "    # hidden: initial hidden state (h0)\n",
        "    def call(self, x, hidden):\n",
        "        # build embeding layer\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # pass embeding layer to GRU\n",
        "        output, state = self.gru(x, initial_state = hidden)        \n",
        "        return output, state\n",
        "    \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n",
        "\n",
        "# calling encoder\n",
        "# vocab_inp_size: len of uniuque words in output language / input size for the GRU network\n",
        "# units: no.of hidden units, it's a hyperparameter\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEk4pChtznAj"
      },
      "source": [
        "#inherit parent class: tf.keras.Model\n",
        "class Decoder(tf.keras.Model):\n",
        "\n",
        "    # input params for the class\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "\n",
        "        # calling parent initiator from it has been inheried\n",
        "        # tf.keras.Model is being iniatiated\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # initiating params\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "\n",
        "        # embeding is the 1st layer of encoder network\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # iniating GRU. We can use RNN or LSTM also\n",
        "        # return_sequences: return output of each state i.e., each h0\n",
        "        # return_state: the final context\n",
        "        self.gru = tf.keras.layers.GRU(self.dec_units, \n",
        "                                    return_sequences=True, \n",
        "                                    return_state=True, \n",
        "                                    recurrent_initializer='glorot_uniform')\n",
        "        \n",
        "        # initialization of attention mechanism\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "        # used for attention\n",
        "        # weights for the inputs, as discussed in attention theory\n",
        "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
        "\n",
        "        # output of attention\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "        \n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # enc_output shape: (batch_size, max_length, hidden_size)\n",
        "        \n",
        "        # hidden shape: (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape: (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score for attention mechanism \n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "        \n",
        "        # score shape: (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
        "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
        "        \n",
        "        # attention_weights shape: (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        \n",
        "        # context_vector shape after sum: (batch_size, hidden_size)\n",
        "        # weighted calculation: multiply weights and hidden state outputs\n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        \n",
        "        # x shape after passing through embedding: (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        # x shape after concatenation: (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        \n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, state = self.gru(x)\n",
        "        \n",
        "        # output shape: (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        \n",
        "        # output shape: (batch_size * 1, vocab)\n",
        "        x = self.fc(output)\n",
        "        \n",
        "        return x, state, attention_weights\n",
        "        \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.dec_units))\n",
        "\n",
        "# calling decoder\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_ke6mQv1bs0"
      },
      "source": [
        "import time\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch, (inp, targ)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_output, enc_hidden = encoder(inp, hidden)\n",
        "            \n",
        "            # input to the decoder network\n",
        "            dec_hidden = enc_hidden\n",
        "            \n",
        "            # triggering 1st input of decoder with <start> tag for each batch\n",
        "            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n",
        "            \n",
        "            # Teacher forcing - feeding the target as the next input\n",
        "            for t in range(1, targ.shape[1]):\n",
        "                # passing enc_output to the decoder\n",
        "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "                \n",
        "                loss += loss_function(targ[:, t], predictions)\n",
        "                \n",
        "                # using teacher forcing\n",
        "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "        \n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        "        \n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        variables = encoder.variables + decoder.variables\n",
        "        \n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        \n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "        \n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.numpy()))\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    \n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}